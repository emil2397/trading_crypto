{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2I-6xYyVSwRj"
   },
   "outputs": [],
   "source": [
    "# ! pip install pyserial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jxhG2qYW4n1M"
   },
   "outputs": [],
   "source": [
    "# ! pip install --upgrade gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import gspread\n",
    "import itertools\n",
    "import imaplib, serial, struct, time\n",
    "import re\n",
    "from email.header import Header, decode_header, make_header\n",
    "import json\n",
    "import collections.abc\n",
    "import pytz \n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fEX2mnAfM5-M"
   },
   "outputs": [],
   "source": [
    "def bayes_prob(f_ev, s_ev):\n",
    "    \n",
    "    def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
    "        # calculate P(not A)\n",
    "        not_a = 1 - p_a\n",
    "        # calculate P(B)\n",
    "        p_b = p_b_given_a * p_a + p_b_given_not_a * not_a\n",
    "        # calculate P(A|B)\n",
    "        try:\n",
    "            p_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "            return p_a_given_b\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "#     f_ev = \"AI_r0_up\"\n",
    "#     s_ev = \"AI_r2_dn\"\n",
    "\n",
    "    if f_ev.split('_')[-1]!=s_ev.split('_')[-1]:\n",
    "        # P(A)\n",
    "        p_a = probabilities_patterns[s_ev]\n",
    "        # P(B|A)\n",
    "        p_b_given_a = abs(probabilities_patterns[f_ev] - probabilities_patterns[s_ev])\n",
    "        # P(B|not A)\n",
    "        p_b_given_not_a = probabilities_patterns[f_ev]\n",
    "        # calculate P(A|B)\n",
    "\n",
    "        result = round(bayes_theorem(p_a, p_b_given_a, p_b_given_not_a),2)\n",
    "        \n",
    "#         probabilities_patterns[s_ev]=result\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "\n",
    "        # P(B|A)\n",
    "        result = round(min(1,probabilities_patterns[f_ev] + probabilities_patterns[s_ev]),2)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ROk1l_DhnVyv"
   },
   "outputs": [],
   "source": [
    "tickers = [\n",
    "# \"US100\",\n",
    "# \"TSLA\",\n",
    "# \"FB\",\n",
    "# \"MSFT\",\n",
    "# \"GOOGL\",\n",
    "# \"MSTR\",\n",
    "'BTCUSDT',\n",
    "# \"USDRUB\",\n",
    "]\n",
    "\n",
    "\n",
    "timeframe_coverter = {\n",
    "    \"60\": 24*60,\n",
    "    \"30\":60,\n",
    "    \"15\":30,\n",
    "    \"5\":15,\n",
    "    \"1\":5\n",
    "}\n",
    "\n",
    "timeframes = timeframe_coverter.keys()\n",
    "\n",
    "timeframes = [str(x) for x in timeframes]\n",
    "\n",
    "# bayes proba\n",
    "patterns_pairs = { key: [] for key in  timeframe_coverter.keys()}\n",
    "patterns_pairs_signals = {ticker: patterns_pairs for ticker in tickers}\n",
    "\n",
    "up_dn_proba = {key: {\"up\": 0, \"dn\": 0 } for key in  timeframe_coverter.keys()}\n",
    "\n",
    "tickers_up_dn_proba = {ticker:up_dn_proba for ticker in tickers}\n",
    "\n",
    "represent_patterns = [\n",
    "\"frequency\",\n",
    "\n",
    "\"ticker_name\",\n",
    "\n",
    "\"Cross\",\n",
    "\"AI_ri_up\", \n",
    "\"AF_up\",\n",
    "\n",
    "\"AI_ri_dn\",\n",
    "\"AF_dn\",\n",
    "\n",
    "\"score_up_points\",\n",
    "\"score_dn_points\",\n",
    "\"score_gen_points\",\n",
    "    \n",
    "\n",
    "\"trend_up_proba\",\n",
    "\"trend_dn_proba\",\n",
    "]\n",
    "\n",
    "patterns = [\n",
    "\"AF_up\",\n",
    "\"AF_dn\",\n",
    "    \n",
    "\"Cross\",\n",
    "    \n",
    "\"Cross_up\", # это не про лонг-шорт, это про пересечение над или под\n",
    "\"Cross_dn\",\n",
    "\n",
    "\"AI_ri_up\",\n",
    "\"AI_ri_dn\",\n",
    "\n",
    "\"AI_r2_dn\",\n",
    "\"AI_r1_dn\",\n",
    "\"AI_r0_dn\",\n",
    "\n",
    "\"AI_r2_up\",\n",
    "\"AI_r1_up\",\n",
    "\"AI_r0_up\",\n",
    "    \n",
    "\"trend_up_proba\",\n",
    "\"trend_dn_proba\",\n",
    "]\n",
    "\n",
    "\n",
    "probabilities_patterns = {\n",
    "    \n",
    "\"Cross_up\": 0.9, \n",
    "\"Cross_dn\": 0.9,\n",
    "\n",
    "\"AI_r2_dn\":0.1,\n",
    "\"AI_r1_dn\":0.2,\n",
    "\"AI_r0_dn\":0.7,\n",
    "\n",
    "\"AI_r2_up\":0.1,\n",
    "\"AI_r1_up\":0.2,\n",
    "\"AI_r0_up\":0.7,\n",
    "}\n",
    "\n",
    "\n",
    "patterns_scores = {\n",
    "    \n",
    "\"Cross_up\": 1, \n",
    "\"Cross_dn\": -1,\n",
    "\n",
    "\"AI_r2_dn\":-0.5,\n",
    "\"AI_r1_dn\":-1,\n",
    "\"AI_r0_dn\":-2,\n",
    "\n",
    "\"AI_r2_up\":0.1,\n",
    "\"AI_r1_up\":0.2,\n",
    "\"AI_r0_up\":0.7,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "user = \"trading.view.alerts777@gmail.com\"\n",
    "pwd = \"kaif666aA\"\n",
    "\n",
    "\n",
    "def create_dashbord():\n",
    "    return {x: {y: {z: '' for z in patterns} for y in timeframes} for x in tickers}\n",
    "\n",
    "\n",
    "def create_dashbord_timer():\n",
    "    return {x: {y: {z: datetime.datetime.now() for z in patterns} for y in timeframes} for x in tickers}\n",
    "\n",
    "\n",
    "class Mail():\n",
    "    def __init__(self):\n",
    "        self.user= user\n",
    "        self.password= pwd\n",
    "\n",
    "        self.M = imaplib.IMAP4_SSL('imap.gmail.com', '993')\n",
    "        self.M.login(self.user, self.password)\n",
    "        \n",
    "    def checkMail(self):\n",
    "        self.M.select()\n",
    "\n",
    "        self.unRead, self.data = self.M.search(None, '(UNSEEN SUBJECT \"alert\")')\n",
    "\n",
    "        if len(self.data[0].split()) > 0:\n",
    "          self.M.store(self.data[0].decode('utf-8').replace(' ',','),'+FLAGS','\\Seen')\n",
    "          return self.data\n",
    "        else:\n",
    "          return ''\n",
    "\n",
    "\n",
    "def get_signals(dashboard):\n",
    "    \n",
    "    new_data=[]\n",
    "\n",
    "    a=email.checkMail()\n",
    "    act=False\n",
    "\n",
    "    if str(a)!='':\n",
    "\n",
    "        if len(a[0].split())>0:\n",
    "\n",
    "            now_date=datetime.datetime.now().strftime('%d %b %Y %X')\n",
    "            now_date=datetime.datetime.strptime(now_date,'%d %b %Y %X')\n",
    "\n",
    "            for i in a[0].split():  \n",
    "\n",
    "                try:\n",
    "                    message_text = str(make_header(decode_header(str(email.M.fetch(i,\"(envelope)\")[1][0]))))\n",
    "                    corpus=re.findall('Оповещение:.*,.*,.{1,10} \\\"{1}', message_text)[0]\n",
    "\n",
    "                    if len(corpus.split(','))>3:\n",
    "\n",
    "                        ticker = corpus.split(',')[1].rstrip().lstrip()\n",
    "                        timeframe = corpus.split(',')[2].rstrip().lstrip()\n",
    "                        pattern = corpus.split(',')[3].replace('\"','').rstrip().lstrip()\n",
    "                    \n",
    "                \n",
    "\n",
    "                    date_message=(re.findall(\"[0-9]*[0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]\", \n",
    "                                             message_text)[0])\n",
    "                    \n",
    "                    date_message=datetime.datetime.strptime(date_message, '%d %b %Y %X')\n",
    "\n",
    "                    date_message+= timedelta(hours=3)\n",
    "\n",
    "                    how_late_mes = (now_date-date_message).total_seconds()/60\n",
    "                    \n",
    "                    if how_late_mes<=timeframe_coverter[timeframe]:\n",
    "                        new_data.append((ticker,timeframe,pattern))\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        act=len(new_data)>0\n",
    "        \n",
    "        if act:\n",
    "\n",
    "            for ticker,timeframe,pattern in new_data:\n",
    "                if str(pattern).__contains__(\"AI_r\"):\n",
    "                    \n",
    "                    if pattern.split('_')[-1]==\"up\":\n",
    "                        dashboard[ticker][timeframe][\"AI_ri_up\"] = pattern\n",
    "\n",
    "                    if pattern.split('_')[-1]==\"dn\":\n",
    "                        dashboard[ticker][timeframe][\"AI_ri_dn\"] = pattern\n",
    "\n",
    "                if str(pattern).__contains__(\"Cross\"):\n",
    "                    dashboard[ticker][timeframe][\"Cross\"] = pattern\n",
    "                \n",
    "                if pattern in probabilities_patterns.keys():\n",
    "                    \n",
    "                    patterns_pairs_signals[ticker][timeframe].append(pattern)\n",
    "                    \n",
    "                    if len(patterns_pairs_signals[ticker][timeframe])>=2:\n",
    "                        del patterns_pairs_signals[ticker][timeframe][0]\n",
    "\n",
    "                    if len(patterns_pairs_signals[ticker][timeframe])==2:\n",
    "                        if pattern.__contains__(\"up\"):\n",
    "                            dashboard[ticker][timeframe][\"trend_up_proba\"]=bayes_prob(patterns_pairs_signals[ticker][timeframe][0],\n",
    "                                                                                      patterns_pairs_signals[ticker][timeframe][1])\n",
    "\n",
    "                            dashboard[ticker][timeframe][\"trend_dn_proba\"]=1-dashboard[ticker][timeframe][\"trend_up_proba\"]\n",
    "\n",
    "                        if pattern.__contains__(\"dn\"):\n",
    "                            dashboard[ticker][timeframe][\"trend_dn_proba\"]=bayes_prob(patterns_pairs_signals[ticker][timeframe][0],\n",
    "                                                                                      patterns_pairs_signals[ticker][timeframe][1])\n",
    "                            dashboard[ticker][timeframe][\"trend_up_proba\"]=1-dashboard[ticker][timeframe][\"trend_dn_proba\"]\n",
    "\n",
    "                dashboard[ticker][timeframe][pattern] = pattern\n",
    "                dashboard_timer[ticker][timeframe][pattern] = datetime.datetime.now()\n",
    "    \n",
    "\n",
    "    return (dashboard,dashboard_timer,act)\n",
    "\n",
    "\n",
    "def update_d(d, u):\n",
    "    for k, v in u.items():\n",
    "        if isinstance(v, collections.abc.Mapping):\n",
    "            d[k] = update_d(d.get(k, {}), v)\n",
    "        else:\n",
    "            d[k] = v\n",
    "    return d\n",
    "\n",
    "\n",
    "# creating tickers sample\n",
    "dashboard_first = create_dashbord()\n",
    "dashboard_timer = create_dashbord_timer()\n",
    "\n",
    "# reading the email box\n",
    "email = Mail()\n",
    "\n",
    "\n",
    "## access to google sheets\n",
    "google_key = {\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"hazel-freehold-344516\",\n",
    "  \"private_key_id\": \"13e6e210fcaafc3399005ca8cd663d6a4ce4faa0\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDuGEK+E82ZqCRu\\n5cMffN74mEyqJc+HvqlyeAynwIJXftyysZWb6gC9CA1gU4Uw1z4lhYUyQCCafo5k\\nXmqNVYG/2OL76aUE4qDbGZEwzxgXJcwU2S0Cq7KSpsIS33e6JNFdfA5qr90bz32F\\nkn2D6L6sR9+7QARVQxVNaS1bB8FPmxYfllTG6nMjYu752cpkvZd+bZuOlI04qeor\\neiZZLcPNbY94dGsWFLHo2eVshkdpE+WZ1taYjVt3s2XXaJl/WF9cfJMNCiCkmJQV\\nBdKzrlAYlTn4lp+cR6+sW07MsafMGlUe98SO9b2tu965G3il4Jci6trpAAvmsJzc\\nwi4bDJwNAgMBAAECggEALxEttHyessb5+NBD2+gn/dXBpgsWdPu8bIwN2GWmfpxu\\nDYcsj6dZIQVD+6xXEpoiR3GXLKFfsJrfFDlT7/+cyCucZ3c/L3GLofRybscpuH24\\n09BA5RvDD9mWWPvDI9Geb9AT24RLffFtG2gjlt1+P/lvYYlcsewyfFyT4kBstwsM\\nVWFPehxqyavYsrzsQ4IYtZr3iHS4UB8ZRYon5B7ketCet/ZuBfCdWVqmSfjzZ7EO\\nmmijLz8/OCRroKDqxNu9P2yP7bW8B4zDojIq8kK8Vh3aNdVlTpOMGBgi0N1pPDyd\\niza8dZM7S4PDAyVec54SUuuRccfiKW3J1rI1NTNfkwKBgQD9sPZ2KMPLHnbqt6Sn\\n9oaZdRUuAU7k4JCcqLCmshME2ezgSPcOjH+lvEL4ZWulI1I19XvkyNq/BqwXIpyY\\nFMtDi6Y7VFHXfSQr4DD4vYpFSJGf4Cpzwalev0GGpssGXamitrxamqh42RWKp9w7\\n8gVav3dfY1Py+979VBmxQ3u62wKBgQDwQvZIXVw3uMErde/L+2wSuJ9fkDYZ6F4y\\nrJSxCYr9hFdTeltxnS5triVLIdCTdafLdG7AiERPzgQe6nRDdVmQD3DHiaiISBKU\\n8ZUX11dgbOoJNjfsWJdJwCHlIsjC7nU++5ijTSiWS8qMPyle7VPmsxsHn4gTeiOD\\nU6k7BgKVNwKBgQD4W2k9DfV4AXALoxM4N9lXnE7Kxg8VdF8+bsrZtpV19161x9jN\\nznAcsayifq+ecHDIUHYk6Rl1T7Pjxkelfx3rF2j6xjaFDob9yTJIIU6fO0cNTChj\\nQKFuFzCwANPbfJBYsiq9TJFIFcXIA0NREEN7rtvPb288/qU0PkQUzOrxZwKBgQCV\\nBWOOpOGS6jReSYtPoQV6Yyru8hqsXRU4JxUe7cVY02H6tBTN1mk6vF4DSNj+7WYC\\n6pfbMWn1ednSdydfxASmNv2Dth1jUhi8a09Hd1iJxWQwDIIJRTWpF9OuNLIDPjZY\\nt2MIPs5i0mWyMWbuqxN0LzkftFKJiiNGzHhV1Ld7ZwKBgQCyy7Ag7Ws58U1QTkD2\\nkjmnNNU1Qm9Tum0kiUFtk+KJ3l6lCe4ulDWWigTuX+MMAogQDB6rKx4TDPDgidrZ\\n4i2uVGbxdc35oKWJa9+vY8x3lCvDEnqzUDt8+P9vkegDWUJhYFhUxSrzIySxL9hc\\n+dUX4hMSFeQpJSyGx5jhfwssmg==\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"trader-viewer@hazel-freehold-344516.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"115495437317317039318\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/trader-viewer%40hazel-freehold-344516.iam.gserviceaccount.com\"\n",
    "}\n",
    "\n",
    "with open('google_key.json', 'w') as outfile:\n",
    "    json.dump(google_key, outfile)\n",
    "\n",
    "creds = gspread.service_account(filename='google_key.json')\n",
    "\n",
    "sh = creds.open(\"trade_dashb\")\n",
    "worksheet = sh.get_worksheet(0)\n",
    "\n",
    "worksheet2 = sh.get_worksheet(1)\n",
    "\n",
    "# that is for log of last time active signal\n",
    "all_combs = list(set(itertools.product(*[tickers, timeframes, patterns])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "1. исправить, чтобы вероятность считалась с 1-го события\n",
    "2. вероятности почеленджить\n",
    "3. почистить нули\n",
    "4. сброс при равных значениях в очках\n",
    "5. убрать мигалки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "dashboard_ = copy.copy(dashboard_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVyTScGN5szc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while 1:\n",
    "    try:\n",
    "\n",
    "        dashboard_, dashboard_timer, do_update = get_signals(dashboard_)\n",
    "        \n",
    "        \n",
    "        if do_update:\n",
    "            now_date = datetime.datetime.now()\n",
    "\n",
    "            ([ update_d(dashboard_, {tc:{tf:{pt:''}}})  for tc,tf,pt in all_combs\n",
    "              if (now_date - dashboard_timer[tc][tf][pt]).total_seconds()/60 > timeframe_coverter[tf]]) ;\n",
    "        \n",
    "#         for tc,tf,pt in all_combs:\n",
    "#             if (now_date - dashboard_timer[tc][tf][pt]).total_seconds()/60 > timeframe_coverter[tf]:\n",
    "#                 dashboard_[tc][tf][pt]=''\n",
    "\n",
    "            dashboard_df = pd.DataFrame.from_dict(dashboard_,orient='index')\n",
    "\n",
    "            dashboard_df = pd.DataFrame.from_dict({(i,j): dashboard_df[i][j] \n",
    "                                        for i in dashboard_df.keys() \n",
    "                                        for j in dashboard_df[i].keys()},\n",
    "                                    orient='index')\n",
    "\n",
    "\n",
    "\n",
    "            # points\n",
    "            dashboard_df.loc[:,\"score_up_points\"]=(dashboard_df[patterns_scores.keys()]\n",
    "                                                   .replace('',0).replace(patterns_scores).sum(axis=1))\n",
    "\n",
    "            dashboard_df.loc[:,\"score_dn_points\"]=(dashboard_df[patterns_scores.keys()]\n",
    "                                                .replace('',0).replace(patterns_scores).sum(axis=1))\n",
    "\n",
    "            dashboard_df.loc[:,\"score_gen_points\"]=dashboard_df[\"score_up_points\"] + dashboard_df[\"score_dn_points\"]\n",
    "\n",
    "\n",
    "\n",
    "            df=dashboard_df.reset_index().rename(columns={\"level_0\":\"frequency\",\"level_1\":\"ticker_name\"})\n",
    "            \n",
    "            (worksheet.update([df[represent_patterns].columns.values.tolist()] \n",
    "                                  + df[represent_patterns].values.tolist()))\n",
    "\n",
    "            (worksheet2.update([df[represent_patterns].columns.values.tolist()] \n",
    "                               + df[represent_patterns].sort_values(by='score_gen_points', ascending=False).values.tolist()))\n",
    "        \n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "    except:\n",
    "        time.sleep(60)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "trading_dashb.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
