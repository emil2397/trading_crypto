{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2I-6xYyVSwRj"
   },
   "outputs": [],
   "source": [
    "# ! pip install pyserial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jxhG2qYW4n1M"
   },
   "outputs": [],
   "source": [
    "# ! pip install --upgrade gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !  pip3 install jupyter-black jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dashboard\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/18DvutbbnEXwa11ke92VN9DvzLKQbLGwGeuqO_FMCeKE/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api key \"29jwQsChmaHg6hJYbYPoXhwguRt_2X2aW7FcnmNCK8hYud4PA\"\n",
    "# webhook url http://8fae-2003-ed-9700-8819-9d11-97f0-6cd-9e25.ngrok.io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from flask import Flask,request,json\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# @app.route('/', methods=['POST'])\n",
    "# def hello():\n",
    "#     if request.method == 'POST':\n",
    "#         request_data = request.get_json()\n",
    "\n",
    "#     return request_data\n",
    " \n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host='0.0.0.0', port=72, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/githubIssue',methods=['POST'])\n",
    "# def githubIssue():\n",
    "#     data = request.json\n",
    "#     print(f'Issue {data[\"issue\"][\"title\"]} {data[\"action\"]}')\n",
    "#     print(f'{data[\"issue\"][\"body\"]}')\n",
    "#     print(f'{data[\"issue\"][\"url\"]}')\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import socket\n",
    "\n",
    "# server = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "# server.bind((\"localhost\", 9999))\n",
    "\n",
    "# server.listen()\n",
    "\n",
    "# while True:\n",
    "#     client, addr = server.accept()\n",
    "#     print(client.recv(1024).decode())\n",
    "#     client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "server = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "server.connect((\"https://fb1c-85-146-180-89.eu.ngrok.io\", 1234))\n",
    "\n",
    "server.listen()\n",
    "\n",
    "while True:\n",
    "    print(client.recv(1024).decode())\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import gspread\n",
    "import itertools\n",
    "import imaplib, serial, struct, time\n",
    "import re\n",
    "from email.header import Header, decode_header, make_header\n",
    "import json\n",
    "import collections.abc\n",
    "import pytz\n",
    "from datetime import timedelta\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## access to google sheets\n",
    "def connect_to_google_sheet():\n",
    "    google_key = {\n",
    "        \"type\": \"service_account\",\n",
    "        \"project_id\": \"hazel-freehold-344516\",\n",
    "        \"private_key_id\": \"13e6e210fcaafc3399005ca8cd663d6a4ce4faa0\",\n",
    "        \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDuGEK+E82ZqCRu\\n5cMffN74mEyqJc+HvqlyeAynwIJXftyysZWb6gC9CA1gU4Uw1z4lhYUyQCCafo5k\\nXmqNVYG/2OL76aUE4qDbGZEwzxgXJcwU2S0Cq7KSpsIS33e6JNFdfA5qr90bz32F\\nkn2D6L6sR9+7QARVQxVNaS1bB8FPmxYfllTG6nMjYu752cpkvZd+bZuOlI04qeor\\neiZZLcPNbY94dGsWFLHo2eVshkdpE+WZ1taYjVt3s2XXaJl/WF9cfJMNCiCkmJQV\\nBdKzrlAYlTn4lp+cR6+sW07MsafMGlUe98SO9b2tu965G3il4Jci6trpAAvmsJzc\\nwi4bDJwNAgMBAAECggEALxEttHyessb5+NBD2+gn/dXBpgsWdPu8bIwN2GWmfpxu\\nDYcsj6dZIQVD+6xXEpoiR3GXLKFfsJrfFDlT7/+cyCucZ3c/L3GLofRybscpuH24\\n09BA5RvDD9mWWPvDI9Geb9AT24RLffFtG2gjlt1+P/lvYYlcsewyfFyT4kBstwsM\\nVWFPehxqyavYsrzsQ4IYtZr3iHS4UB8ZRYon5B7ketCet/ZuBfCdWVqmSfjzZ7EO\\nmmijLz8/OCRroKDqxNu9P2yP7bW8B4zDojIq8kK8Vh3aNdVlTpOMGBgi0N1pPDyd\\niza8dZM7S4PDAyVec54SUuuRccfiKW3J1rI1NTNfkwKBgQD9sPZ2KMPLHnbqt6Sn\\n9oaZdRUuAU7k4JCcqLCmshME2ezgSPcOjH+lvEL4ZWulI1I19XvkyNq/BqwXIpyY\\nFMtDi6Y7VFHXfSQr4DD4vYpFSJGf4Cpzwalev0GGpssGXamitrxamqh42RWKp9w7\\n8gVav3dfY1Py+979VBmxQ3u62wKBgQDwQvZIXVw3uMErde/L+2wSuJ9fkDYZ6F4y\\nrJSxCYr9hFdTeltxnS5triVLIdCTdafLdG7AiERPzgQe6nRDdVmQD3DHiaiISBKU\\n8ZUX11dgbOoJNjfsWJdJwCHlIsjC7nU++5ijTSiWS8qMPyle7VPmsxsHn4gTeiOD\\nU6k7BgKVNwKBgQD4W2k9DfV4AXALoxM4N9lXnE7Kxg8VdF8+bsrZtpV19161x9jN\\nznAcsayifq+ecHDIUHYk6Rl1T7Pjxkelfx3rF2j6xjaFDob9yTJIIU6fO0cNTChj\\nQKFuFzCwANPbfJBYsiq9TJFIFcXIA0NREEN7rtvPb288/qU0PkQUzOrxZwKBgQCV\\nBWOOpOGS6jReSYtPoQV6Yyru8hqsXRU4JxUe7cVY02H6tBTN1mk6vF4DSNj+7WYC\\n6pfbMWn1ednSdydfxASmNv2Dth1jUhi8a09Hd1iJxWQwDIIJRTWpF9OuNLIDPjZY\\nt2MIPs5i0mWyMWbuqxN0LzkftFKJiiNGzHhV1Ld7ZwKBgQCyy7Ag7Ws58U1QTkD2\\nkjmnNNU1Qm9Tum0kiUFtk+KJ3l6lCe4ulDWWigTuX+MMAogQDB6rKx4TDPDgidrZ\\n4i2uVGbxdc35oKWJa9+vY8x3lCvDEnqzUDt8+P9vkegDWUJhYFhUxSrzIySxL9hc\\n+dUX4hMSFeQpJSyGx5jhfwssmg==\\n-----END PRIVATE KEY-----\\n\",\n",
    "        \"client_email\": \"trader-viewer@hazel-freehold-344516.iam.gserviceaccount.com\",\n",
    "        \"client_id\": \"115495437317317039318\",\n",
    "        \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "        \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "        \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "        \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/trader-viewer%40hazel-freehold-344516.iam.gserviceaccount.com\",\n",
    "    }\n",
    "\n",
    "    with open(\"google_key.json\", \"w\") as outfile:\n",
    "        json.dump(google_key, outfile)\n",
    "\n",
    "    creds = gspread.service_account(filename=\"google_key.json\")\n",
    "\n",
    "    sh = creds.open(\"trade_dashb\")\n",
    "    worksheet = sh.get_worksheet(0)\n",
    "\n",
    "    worksheet2 = sh.get_worksheet(1)\n",
    "\n",
    "    return worksheet, worksheet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fEX2mnAfM5-M",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class stats:\n",
    "    def bayes_prob(f_ev, s_ev):\n",
    "        def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
    "            # calculate P(not A)\n",
    "            not_a = 1 - p_a\n",
    "            # calculate P(B)\n",
    "            p_b = p_b_given_a * p_a + p_b_given_not_a * not_a\n",
    "            # calculate P(A|B)\n",
    "            try:\n",
    "                p_a_given_b = (p_b_given_a * p_a) / p_b\n",
    "                return p_a_given_b\n",
    "            except:\n",
    "                return 0\n",
    "\n",
    "        #     f_ev = \"AI_r0_up\"\n",
    "        #     s_ev = \"AI_r2_dn\"\n",
    "\n",
    "        if f_ev.split(\"_\")[-1] != s_ev.split(\"_\")[-1]:\n",
    "            # P(A)\n",
    "            p_a = probabilities_patterns[s_ev]\n",
    "            # P(B|A)\n",
    "            p_b_given_a = abs(\n",
    "                probabilities_patterns[f_ev] - probabilities_patterns[s_ev]\n",
    "            )\n",
    "            # P(B|not A)\n",
    "            p_b_given_not_a = probabilities_patterns[f_ev]\n",
    "            # calculate P(A|B)\n",
    "\n",
    "            result = round(bayes_theorem(p_a, p_b_given_a, p_b_given_not_a), 2)\n",
    "\n",
    "            #         probabilities_patterns[s_ev]=result\n",
    "\n",
    "            return result\n",
    "        else:\n",
    "\n",
    "            # P(B|A)\n",
    "            result = round(\n",
    "                min(1, probabilities_patterns[f_ev] + probabilities_patterns[s_ev]), 2\n",
    "            )\n",
    "\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ROk1l_DhnVyv"
   },
   "outputs": [],
   "source": [
    "def create_data():\n",
    "    \n",
    "    def create_dashbord():\n",
    "        return {x: {y: {z: \"\" for z in patterns} for y in timeframes} for x in tickers}\n",
    "\n",
    "\n",
    "    def create_dashbord_timer():\n",
    "    \n",
    "        return {\n",
    "            x: {y: {z: datetime.datetime.now() for z in patterns} for y in timeframes}\n",
    "            for x in tickers\n",
    "        }\n",
    "\n",
    "    tickers = [\n",
    "        # \"US100\",\n",
    "        # \"TSLA\",\n",
    "        # \"FB\",\n",
    "        # \"MSFT\",\n",
    "        # \"GOOGL\",\n",
    "        # \"MSTR\",\n",
    "        \"BTCUSDT\",\n",
    "        # \"USDRUB\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    timeframe_coverter = {\"60\": 24 * 60, \"30\": 60, \"15\": 30, \"5\": 15, \"1\": 5}\n",
    "\n",
    "    timeframes = timeframe_coverter.keys()\n",
    "\n",
    "    timeframes = [str(x) for x in timeframes]\n",
    "\n",
    "    # bayes proba\n",
    "    patterns_pairs = {key: [] for key in timeframe_coverter.keys()}\n",
    "    patterns_pairs_signals = {ticker: patterns_pairs for ticker in tickers}\n",
    "\n",
    "    up_dn_proba = {key: {\"up\": 0, \"dn\": 0} for key in timeframe_coverter.keys()}\n",
    "\n",
    "    tickers_up_dn_proba = {ticker: up_dn_proba for ticker in tickers}\n",
    "\n",
    "    represent_patterns = [\n",
    "        \"frequency\",\n",
    "        \"ticker_name\",\n",
    "        \"Cross\",\n",
    "        \"AI_ri_up\",\n",
    "        \"AF_up\",\n",
    "        \"AI_ri_dn\",\n",
    "        \"AF_dn\",\n",
    "        \"score_up_points\",\n",
    "        \"score_dn_points\",\n",
    "        \"score_gen_points\",\n",
    "        \"trend_up_proba\",\n",
    "        \"trend_dn_proba\",\n",
    "    ]\n",
    "\n",
    "    patterns = [\n",
    "        \"AF_up\",\n",
    "        \"AF_dn\",\n",
    "        \"Cross\",\n",
    "        \"Cross_up\",  # это не про лонг-шорт, это про пересечение над или под\n",
    "        \"Cross_dn\",\n",
    "        \"AI_ri_up\",\n",
    "        \"AI_ri_dn\",\n",
    "        \"AI_r2_dn\",\n",
    "        \"AI_r1_dn\",\n",
    "        \"AI_r0_dn\",\n",
    "        \"AI_r2_up\",\n",
    "        \"AI_r1_up\",\n",
    "        \"AI_r0_up\",\n",
    "        \"trend_up_proba\",\n",
    "        \"trend_dn_proba\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    probabilities_patterns = {\n",
    "        \"Cross_up\": 0.9,\n",
    "        \"Cross_dn\": 0.9,\n",
    "        \"AI_r2_dn\": 0.1,\n",
    "        \"AI_r1_dn\": 0.2,\n",
    "        \"AI_r0_dn\": 0.7,\n",
    "        \"AI_r2_up\": 0.1,\n",
    "        \"AI_r1_up\": 0.2,\n",
    "        \"AI_r0_up\": 0.7,\n",
    "    }\n",
    "\n",
    "\n",
    "    patterns_scores = {\n",
    "        \"Cross_up\": 1,\n",
    "        \"Cross_dn\": -1,\n",
    "        \"AI_r2_dn\": -0.5,\n",
    "        \"AI_r1_dn\": -1,\n",
    "        \"AI_r0_dn\": -2,\n",
    "        \"AI_r2_up\": 0.1,\n",
    "        \"AI_r1_up\": 0.2,\n",
    "        \"AI_r0_up\": 0.7,\n",
    "    }\n",
    "    # creating tickers sample\n",
    "    dashboard_first = create_dashbord()\n",
    "    dashboard_timer = create_dashbord_timer()\n",
    "    \n",
    "    # that is for log of last time active signal\n",
    "    all_combs = list(set(itertools.product(*[tickers, timeframes, patterns])))\n",
    "\n",
    "    return dashboard_first,dashboard_timer,all_combs\n",
    "\n",
    "class Mail:\n",
    "    \n",
    "    user = \"trading.view.alerts777@gmail.com\"\n",
    "    pwd = \"kaif666aA\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user = user\n",
    "        self.password = pwd\n",
    "\n",
    "        self.M = imaplib.IMAP4_SSL(\"imap.gmail.com\", \"993\")\n",
    "        self.M.login(self.user, self.password)\n",
    "\n",
    "    def checkMail(self):\n",
    "        self.M.select()\n",
    "\n",
    "        self.unRead, self.data = self.M.search(None, '(UNSEEN SUBJECT \"alert\")')\n",
    "\n",
    "        if len(self.data[0].split()) > 0:\n",
    "            self.M.store(\n",
    "                self.data[0].decode(\"utf-8\").replace(\" \", \",\"), \"+FLAGS\", \"\\Seen\"\n",
    "            )\n",
    "            return self.data\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def delete_mail():\n",
    "        self.M.select()\n",
    "        self.M.expunge()\n",
    "\n",
    "\n",
    "def get_signals(dashboard):\n",
    "\n",
    "    new_data = []\n",
    "\n",
    "    a = email.checkMail()\n",
    "    act = False\n",
    "\n",
    "    if str(a) != \"\":\n",
    "\n",
    "        if len(a[0].split()) > 0:\n",
    "\n",
    "            now_date = datetime.datetime.now().strftime(\"%d %b %Y %X\")\n",
    "            now_date = datetime.datetime.strptime(now_date, \"%d %b %Y %X\")\n",
    "\n",
    "            for i in a[0].split():\n",
    "\n",
    "                try:\n",
    "                    message_text = str(\n",
    "                        make_header(\n",
    "                            decode_header(str(email.M.fetch(i, \"(envelope)\")[1][0]))\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    corpus = re.findall('Оповещение:.*,.*,.{1,10} \"{1}', message_text)[\n",
    "                        0\n",
    "                    ]\n",
    "\n",
    "                    print(corpus)\n",
    "\n",
    "                    if len(corpus.split(\",\")) > 3:\n",
    "\n",
    "                        ticker = corpus.split(\",\")[1].rstrip().lstrip()\n",
    "                        timeframe = corpus.split(\",\")[2].rstrip().lstrip()\n",
    "                        pattern = (\n",
    "                            corpus.split(\",\")[3].replace('\"', \"\").rstrip().lstrip()\n",
    "                        )\n",
    "\n",
    "                    date_message = re.findall(\n",
    "                        \"[0-9]*[0-9] [A-Z][a-z][a-z] [0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]:[0-9][0-9]\",\n",
    "                        message_text,\n",
    "                    )[0]\n",
    "\n",
    "                    date_message = datetime.datetime.strptime(\n",
    "                        date_message, \"%d %b %Y %X\"\n",
    "                    )\n",
    "\n",
    "                    date_message += timedelta(hours=3)\n",
    "\n",
    "                    how_late_mes = (now_date - date_message).total_seconds() / 60\n",
    "\n",
    "                    if how_late_mes <= timeframe_coverter[timeframe]:\n",
    "                        new_data.append((ticker, timeframe, pattern))\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        act = len(new_data) > 0\n",
    "\n",
    "        if act:\n",
    "\n",
    "            for ticker, timeframe, pattern in new_data:\n",
    "                if str(pattern).__contains__(\"AI_r\"):\n",
    "\n",
    "                    if pattern.split(\"_\")[-1] == \"up\":\n",
    "                        dashboard[ticker][timeframe][\"AI_ri_up\"] = pattern\n",
    "\n",
    "                    if pattern.split(\"_\")[-1] == \"dn\":\n",
    "                        dashboard[ticker][timeframe][\"AI_ri_dn\"] = pattern\n",
    "\n",
    "                if str(pattern).__contains__(\"Cross\"):\n",
    "                    dashboard[ticker][timeframe][\"Cross\"] = pattern\n",
    "\n",
    "                if pattern in probabilities_patterns.keys():\n",
    "\n",
    "                    patterns_pairs_signals[ticker][timeframe].append(pattern)\n",
    "\n",
    "                    if len(patterns_pairs_signals[ticker][timeframe]) >= 2:\n",
    "                        del patterns_pairs_signals[ticker][timeframe][0]\n",
    "\n",
    "                    if len(patterns_pairs_signals[ticker][timeframe]) == 2:\n",
    "                        if pattern.__contains__(\"up\"):\n",
    "                            dashboard[ticker][timeframe][\"trend_up_proba\"] = bayes_prob(\n",
    "                                patterns_pairs_signals[ticker][timeframe][0],\n",
    "                                patterns_pairs_signals[ticker][timeframe][1],\n",
    "                            )\n",
    "\n",
    "                            dashboard[ticker][timeframe][\"trend_dn_proba\"] = (\n",
    "                                1 - dashboard[ticker][timeframe][\"trend_up_proba\"]\n",
    "                            )\n",
    "\n",
    "                        if pattern.__contains__(\"dn\"):\n",
    "                            dashboard[ticker][timeframe][\"trend_dn_proba\"] = bayes_prob(\n",
    "                                patterns_pairs_signals[ticker][timeframe][0],\n",
    "                                patterns_pairs_signals[ticker][timeframe][1],\n",
    "                            )\n",
    "                            dashboard[ticker][timeframe][\"trend_up_proba\"] = (\n",
    "                                1 - dashboard[ticker][timeframe][\"trend_dn_proba\"]\n",
    "                            )\n",
    "\n",
    "                dashboard[ticker][timeframe][pattern] = pattern\n",
    "                dashboard_timer[ticker][timeframe][pattern] = datetime.datetime.now()\n",
    "\n",
    "    return (dashboard, dashboard_timer, act)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install imapclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the email box\n",
    "email = Mail()\n",
    "worksheet, worksheet2 = connect_to_google_sheet()\n",
    "dashboard_first,dashboard_timer,all_combs = create_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "1. исправить, чтобы вероятность считалась с 1-го события\n",
    "2. убрать мигалки\n",
    "3. сука, упростить это гавно\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "dashboard_ = copy.copy(dashboard_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dashboard(dashboard_, dashboard_timer):\n",
    "    def update_time(d, u):\n",
    "        for k, v in u.items():\n",
    "            if isinstance(v, collections.abc.Mapping):\n",
    "                d[k] = update_d(d.get(k, {}), v)\n",
    "            else:\n",
    "                d[k] = v\n",
    "        return d\n",
    "\n",
    "    (\n",
    "        [\n",
    "            update_d(dashboard_, {tc: {tf: {pt: \"\"}}})\n",
    "            for tc, tf, pt in all_combs\n",
    "            if (now_date - dashboard_timer[tc][tf][pt]).total_seconds() / 60\n",
    "            > timeframe_coverter[tf]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return dashboard_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVyTScGN5szc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,AI_r1_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,AI_r0_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,AI_r1_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,AI_r1_dn \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n",
      "\n",
      "Оповещение: alert,BTCUSDT,1,Cross_up \"\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        dashboard_, dashboard_timer, do_update = get_signals(dashboard_)\n",
    "        \n",
    "        if do_update:\n",
    "            \n",
    "            update_dashboard.update_time(dashboard_,dashboard_timer)\n",
    "            \n",
    "            now_date = datetime.datetime.now()\n",
    "\n",
    "            # тут обновление словаря выделил в отдельную пока\n",
    "\n",
    "            dashboard_df = pd.DataFrame.from_dict(dashboard_,orient='index')\n",
    "\n",
    "            dashboard_df = pd.DataFrame.from_dict({(i,j): dashboard_df[i][j] \n",
    "                                        for i in dashboard_df.keys() \n",
    "                                        for j in dashboard_df[i].keys()},\n",
    "                                    orient='index')\n",
    "\n",
    "\n",
    "\n",
    "            # points\n",
    "            dashboard_df.loc[:,\"score_up_points\"]=(dashboard_df[patterns_scores.keys()]\n",
    "                                                   .replace('',0).replace(patterns_scores).sum(axis=1))\n",
    "\n",
    "            dashboard_df.loc[:,\"score_dn_points\"]=(dashboard_df[patterns_scores.keys()]\n",
    "                                                .replace('',0).replace(patterns_scores).sum(axis=1))\n",
    "\n",
    "            dashboard_df.loc[:,\"score_gen_points\"]=dashboard_df[\"score_up_points\"] + dashboard_df[\"score_dn_points\"]\n",
    "\n",
    "\n",
    "            df=dashboard_df.reset_index().rename(columns={\"level_0\":\"frequency\",\"level_1\":\"ticker_name\"})\n",
    "            \n",
    "            (worksheet.update([df[represent_patterns].columns.values.tolist()] \n",
    "                                  + df[represent_patterns].values.tolist()))\n",
    "\n",
    "            (worksheet2.update([df[represent_patterns].columns.values.tolist()] \n",
    "                               + df[represent_patterns].sort_values(by='score_gen_points', ascending=False).values.tolist()))\n",
    "        \n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "    except:\n",
    "        time.sleep(60)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "trading_dashb.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
